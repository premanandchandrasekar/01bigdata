High Level List of Tasks in the project/case study
==================================================

1. Create an EMR cluster
2. Set Up and run Kafka daemons on the master node
3. Create a Kafka topic for publishing the real time data
4. Run the producer (console producer is fine as we have one test record for streaming) on one terminal
5. Run the Spark Streaming with Kafka that will consume the test record from the topic and process it on another terminal

Steps 1, 2 and 3 are straight forward and can be completed with a few commands and steps on the AWS console

Step-4 also can be done directly by running console producer and entering the given test record

Step-5 involves
 A. Developing the machine learning applicaiton and saving the model
 B. Developing the Spark Streaming application with Kafka API to use the above model to process the test record

A. Developing the machine learning applicaiton and saving the model
-------------------------------------------------------------------
Check the input data for arriving at the features and the model
Check for the missing values (You can use .isnull or .isnan for each column in a loop as in the example given earlier)
Logistic Regression can be used for this data and expected outcome
The label or dependent variable is the column named target
(You can rename target as label - as done in the previous Linear Regression example)
Use the following transformations of the input data:
 - StringIndexer
 - One Hot Encoder
 - Vector Assembler
And then Logistic Regression algorithm.
You can put the above including the model into a pipeline if desired, which is a good practice
You can then save the model using the model's .write and .save methods
Similar to a dataframe the model gets saved in a directory

You then need to transfer the model directory to the cluster and to HDFS
(assuming you used PySpark with Jupyter Notebook as the development environment on your laptop)

Now the model is ready to be called from your PySpark Streaming-Kafka application for processing the test record(s)


B. Developing the Spark Streaming application with Kafka API to use the above model to process the test record
--------------------------------------------------------------------------------------------------------------
Create a Spark DStream (you can use KafkaUtils.createStream as it is simpler)
Each DStream is made up of RDDs as you know
So on the RDD that is non-empty containing our test record published, you need to run the model
You can use "foreachRDD" function of DStream for this purpose like <DStream>.foreachRDD(<process>)
Now "process" here will be a function in your streaming application which is defined and coded to do the following:
 (with RDD as the input parameter type)
 - If the input RDD is empty (rdd.isEmpty) then nothing to do so just return
 - If not then
    - Use SparkSession object
      (using spark=SparkSession.builder.getOrCreate(), in case you are running the applicaiton with spark-submit)
      (in case you are running the application line by line in pyspark shell then spark is readily created for you)
    - Convert the RDD into a dataframe (as shown in SparkSQL code example earlier)
    - Load the pipeline model which was saved and transferred to HDFS (use PipelineModel.load method for this)
    - To the transform method of the pipeline model, pass the dataframe
    - Transformed dataframe with prediciton column added will be the output of the above as we know
    - Print the prediciton column of the transformed dataframe using select and show methods
    - That will end the "process" function of our PySpark Streaming application

After the line calling the above "process" method as in <DStream>.foreachRDD(<process>) you can
Start Spark Streaming Context and set it to await termination as in ssc.start() and ssc.awaitTermination()

==========================================================================================================

Tasks in Steps 2,3 and 4 are reproduced here for convenience. Please cross check with the commands file that you used earlier before running these steps.

# Connect to your EMR cluster master note with putty if you are using Windows otherwise with ssh
# Download Kafka tar file
$ wget https://archive.apache.org/dist/kafka/0.10.1.1/kafka_2.11-0.10.1.1.tgz

# Extract files from the tar archive
$ tar -zxvf kafka_2.11-0.10.1.1.tgz

# Rename the directory
$ mv kafka_2.11-0.10.1.1 kafka

$ cd kafka

# Download Kafka Spark Streaming library .jar connector
$ wget https://repo1.maven.org/maven2/org/apache/spark/spark-streaming-kafka-0-8-assembly_2.11/2.4.8/spark-streaming-kafka-0-8-assembly_2.11-2.4.8.jar

===================================================================================================

# Start Zookeeper service and Kafka service with the command below which run them in the background
bin/zookeeper-server-start.sh config/zookeeper.properties > /tmp/zkservinit.log 2>&1 &
bin/kafka-server-start.sh config/server.properties > /tmp/kfservinit.log 2>&1 &

bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 2 --topic case-study

--------------------------------------------New Session---------------------------------------------------------

#start streaming job
spark-submit --master local[2] --jars spark-streaming-kafka-0-8-assembly_2.11-2.4.7.jar SparkStreaming.py

# Or you can use pyspark --masket local[2] and run your code line by line if so desired

#send the following data through kafka producer
Cash loans,F,Y,Y,1,171000.0,1560726.0,41301.0,Commercial associate,Higher education,Married ,House/ apartment,-13778,-3130,1,1,0,1,1,3.0,2,2,0,0,Business Entity Type 3,0,0,0,0,0, 0,1,0,0,0,0